{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import random \n",
    "random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bodies_path = \"fnc-1/train_bodies.csv\"\n",
    "train_headlines_path = \"fnc-1/train_stances.csv\"\n",
    "\n",
    "test_bodies_path = \"fnc-1/competition_test_bodies.csv\"\n",
    "test_headlines_path= \"fnc-1/competition_test_stances.csv\"\n",
    "\n",
    "\n",
    "bodies_df_train = pd.read_csv(train_bodies_path)\n",
    "stances_df_train = pd.read_csv(train_headlines_path)\n",
    "df_train = pd.merge(bodies_df_train, stances_df_train, how='right', on='Body ID')\n",
    "\n",
    "bodies_df_test = pd.read_csv(test_bodies_path)\n",
    "stances_df_test = pd.read_csv(test_headlines_path)\n",
    "df_test = pd.merge(bodies_df_test, stances_df_test, how='right', on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_STOP_WORDS = set([\n",
    "    'a',\n",
    "    '000', '2014',\n",
    "    'about',\n",
    "    'above',\n",
    "    'across',\n",
    "    'after',\n",
    "    'afterwards',\n",
    "    'again',\n",
    "    'ain',\n",
    "    'all',\n",
    "    'almost',\n",
    "    'alone',\n",
    "    'along',\n",
    "    'already',\n",
    "    'also',\n",
    "    'although',\n",
    "    'always',\n",
    "    'am',\n",
    "    'among',\n",
    "    'amongst',\n",
    "    'amoungst',\n",
    "    'amount',\n",
    "    'an',\n",
    "    'and',\n",
    "    'another',\n",
    "    'any',\n",
    "    'anyhow',\n",
    "    'anyone',\n",
    "    'anything',\n",
    "    'anyway',\n",
    "    'anywhere',\n",
    "    'are',\n",
    "    'aren',\n",
    "    'around',\n",
    "    'as',\n",
    "    'at',\n",
    "    'back',\n",
    "    'be',\n",
    "    'became',\n",
    "    'because',\n",
    "    'become',\n",
    "    'becomes',\n",
    "    'becoming',\n",
    "    'been',\n",
    "    'before',\n",
    "    'beforehand',\n",
    "    'behind',\n",
    "    'being',\n",
    "    'below',\n",
    "    'beside',\n",
    "    'besides',\n",
    "    'between',\n",
    "    'beyond',\n",
    "    'bill',\n",
    "    'both',\n",
    "    'bottom',\n",
    "    'but',\n",
    "    'by',\n",
    "    'call',\n",
    "    'co',\n",
    "    'con',\n",
    "    'could',\n",
    "    'couldn',\n",
    "    'couldnt',\n",
    "    'cry',\n",
    "    'd',\n",
    "    'de',\n",
    "    'describe',\n",
    "    'detail',\n",
    "    'did',\n",
    "    'didn',\n",
    "    'do',\n",
    "    'does',\n",
    "    'doesn',\n",
    "    'doing',\n",
    "    'don',\n",
    "    'done',\n",
    "    'down',\n",
    "    'due',\n",
    "    'during',\n",
    "    'each',\n",
    "    'eg',\n",
    "    'eight',\n",
    "    'either',\n",
    "    'eleven',\n",
    "    'else',\n",
    "    'elsewhere',\n",
    "    'empty',\n",
    "    'enough',\n",
    "    'etc',\n",
    "    'even',\n",
    "    'ever',\n",
    "    'every',\n",
    "    'everyone',\n",
    "    'everything',\n",
    "    'everywhere',\n",
    "    'except',\n",
    "    'few',\n",
    "    'fifteen',\n",
    "    'fify',\n",
    "    'fill',\n",
    "    'find',\n",
    "    'fire',\n",
    "    'first',\n",
    "    'five',\n",
    "    'for',\n",
    "    'former',\n",
    "    'formerly',\n",
    "    'forty',\n",
    "    'found',\n",
    "    'four',\n",
    "    'from',\n",
    "    'front',\n",
    "    'full',\n",
    "    'further',\n",
    "    'get',\n",
    "    'give',\n",
    "    'go',\n",
    "    'had',\n",
    "    'hadn',\n",
    "    'has',\n",
    "    'hasn',\n",
    "    'hasnt',\n",
    "    'have',\n",
    "    'haven',\n",
    "    'having',\n",
    "    'he',\n",
    "    'hence',\n",
    "    'her',\n",
    "    'here',\n",
    "    'hereafter',\n",
    "    'hereby',\n",
    "    'herein',\n",
    "    'hereupon',\n",
    "    'hers',\n",
    "    'herself',\n",
    "    'him',\n",
    "    'himself',\n",
    "    'his',\n",
    "    'how',\n",
    "    'however',\n",
    "    'hundred',\n",
    "    'i',\n",
    "    'ie',\n",
    "    'if',\n",
    "    'in',\n",
    "    'inc',\n",
    "    'indeed',\n",
    "    'interest',\n",
    "    'into',\n",
    "    'is',\n",
    "    'isn',\n",
    "    'it',\n",
    "    'its',\n",
    "    'itself',\n",
    "    'just',\n",
    "    'keep',\n",
    "    'last',\n",
    "    'latter',\n",
    "    'latterly',\n",
    "    'least',\n",
    "    'less',\n",
    "    'll',\n",
    "    'ltd',\n",
    "    'm',\n",
    "    'ma',\n",
    "    'made',\n",
    "    'many',\n",
    "    'may',\n",
    "    'me',\n",
    "    'meanwhile',\n",
    "    'might',\n",
    "    'mightn',\n",
    "    'mill',\n",
    "    'mine',\n",
    "    'more',\n",
    "    'moreover',\n",
    "    'most',\n",
    "    'mostly',\n",
    "    'move',\n",
    "    'much',\n",
    "    'must',\n",
    "    'mustn',\n",
    "    'my',\n",
    "    'myself',\n",
    "    'name',\n",
    "    'namely',\n",
    "    'needn',\n",
    "    'neither',\n",
    "    'never',\n",
    "    'nevertheless',\n",
    "    'next',\n",
    "    'nine',\n",
    "    'no',\n",
    "    'nobody',\n",
    "    'none',\n",
    "    'noone',\n",
    "    'nor',\n",
    "    'nothing',\n",
    "    'now',\n",
    "    'nowhere',\n",
    "    'o',\n",
    "    'of',\n",
    "    'off',\n",
    "    'often',\n",
    "    'on',\n",
    "    'once',\n",
    "    'one',\n",
    "    'only',\n",
    "    'onto',\n",
    "    'or',\n",
    "    'other',\n",
    "    'others',\n",
    "    'otherwise',\n",
    "    'our',\n",
    "    'ours',\n",
    "    'ourselves',\n",
    "    'out',\n",
    "    'over',\n",
    "    'own',\n",
    "    'part',\n",
    "    'per',\n",
    "    'perhaps',\n",
    "    'please',\n",
    "    'put',\n",
    "    'rather',\n",
    "    're',\n",
    "    's',\n",
    "    'same',\n",
    "    'see',\n",
    "    'seem',\n",
    "    'seemed',\n",
    "    'seeming',\n",
    "    'seems',\n",
    "    'serious',\n",
    "    'several',\n",
    "    'shan',\n",
    "    'she',\n",
    "    'should',\n",
    "    'shouldn',\n",
    "    'show',\n",
    "    'side',\n",
    "    'since',\n",
    "    'sincere',\n",
    "    'six',\n",
    "    'sixty',\n",
    "    'so',\n",
    "    'some',\n",
    "    'somehow',\n",
    "    'someone',\n",
    "    'something',\n",
    "    'sometime',\n",
    "    'sometimes',\n",
    "    'somewhere',\n",
    "    'still',\n",
    "    'such',\n",
    "    'system',\n",
    "    't',\n",
    "    'take',\n",
    "    'ten',\n",
    "    'than',\n",
    "    'that',\n",
    "    'the',\n",
    "    'their',\n",
    "    'theirs',\n",
    "    'them',\n",
    "    'themselves',\n",
    "    'then',\n",
    "    'thence',\n",
    "    'there',\n",
    "    'thereafter',\n",
    "    'thereby',\n",
    "    'therefore',\n",
    "    'therein',\n",
    "    'thereupon',\n",
    "    'these',\n",
    "    'they',\n",
    "    'thick',\n",
    "    'thin',\n",
    "    'third',\n",
    "    'this',\n",
    "    'those',\n",
    "    'though',\n",
    "    'three',\n",
    "    'through',\n",
    "    'throughout',\n",
    "    'thru',\n",
    "    'thus',\n",
    "    'to',\n",
    "    'together',\n",
    "    'too',\n",
    "    'top',\n",
    "    'toward',\n",
    "    'towards',\n",
    "    'twelve',\n",
    "    'twenty',\n",
    "    'two',\n",
    "    'un',\n",
    "    'under',\n",
    "    'until',\n",
    "    'up',\n",
    "    'upon',\n",
    "    'us',\n",
    "    've',\n",
    "    'very',\n",
    "    'via',\n",
    "    'was',\n",
    "    'wasn',\n",
    "    'we',\n",
    "    'well',\n",
    "    'were',\n",
    "    'weren',\n",
    "    'what',\n",
    "    'whatever',\n",
    "    'when',\n",
    "    'whence',\n",
    "    'whenever',\n",
    "    'where',\n",
    "    'whereafter',\n",
    "    'whereas',\n",
    "    'whereby',\n",
    "    'wherein',\n",
    "    'whereupon',\n",
    "    'wherever',\n",
    "    'whether',\n",
    "    'which',\n",
    "    'while',\n",
    "    'whither',\n",
    "    'who',\n",
    "    'whoever',\n",
    "    'whole',\n",
    "    'whom',\n",
    "    'whose',\n",
    "    'why',\n",
    "    'will',\n",
    "    'with',\n",
    "    'within',\n",
    "    'without',\n",
    "    'won',\n",
    "    'would',\n",
    "    'wouldn',\n",
    "    'y',\n",
    "    'yet',\n",
    "    'you',\n",
    "    'your',\n",
    "    'yours',\n",
    "    'yourself',\n",
    "    'yourselves'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49972, 5000)\n",
      "(49972, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus= df_train['articleBody'] + df_train['Headline']\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS , max_features=5000)\n",
    "TF_vect_fitted = vectorizer.fit(corpus)\n",
    "\n",
    "TF_vect_headline_train = TF_vect_fitted.transform(df_train['Headline'])\n",
    "TF_vect_body_train = TF_vect_fitted.transform(df_train['articleBody'])\n",
    "\n",
    "TF_vect_headline_test = TF_vect_fitted.transform(df_test['Headline'])\n",
    "TF_vect_body_test = TF_vect_fitted.transform(df_test['articleBody'])\n",
    "\n",
    "print(TF_vect_headline_train.shape)\n",
    "print(TF_vect_body_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77972\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "corpus.extend(bodies_df_train['articleBody'])\n",
    "corpus.extend(bodies_df_test['articleBody'])\n",
    "\n",
    "corpus.extend(stances_df_train['Headline'])\n",
    "corpus.extend(stances_df_test['Headline'])\n",
    "\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49972, 5000)\n",
      "(49972, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS , max_features=5000, norm='l2')\n",
    "TF_IDF_vect_fitted = vectorizer.fit(corpus)\n",
    "\n",
    "TF_IDF_vect_headline_train = TF_IDF_vect_fitted.transform(df_train['Headline'])\n",
    "TF_IDF_vect_body_train = TF_IDF_vect_fitted.transform(df_train['articleBody'])\n",
    "\n",
    "TF_IDF_vect_headline_test = TF_IDF_vect_fitted.transform(df_test['Headline'])\n",
    "TF_IDF_vect_body_test = TF_IDF_vect_fitted.transform(df_test['articleBody'])\n",
    "\n",
    "\n",
    "print(TF_IDF_vect_headline_train.shape)\n",
    "print(TF_IDF_vect_body_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_cosine_similarity(TF_IDF_vect_body, TF_IDF_vect_headline):\n",
    "  cosine_scores = []\n",
    "  n = TF_IDF_vect_headline.shape[0]\n",
    "  for i in range(n):\n",
    "    TFIDF_body, TFIDF_heading = TF_IDF_vect_body[i].toarray(), TF_IDF_vect_headline[i].toarray()\n",
    "    c = cosine_similarity(TFIDF_body, TFIDF_heading)[0][0]\n",
    "    cosine_scores.append(c)\n",
    "  return cosine_scores\n",
    "\n",
    "cosine_scores_train = get_cosine_similarity(TF_IDF_vect_body_train, TF_IDF_vect_headline_train)\n",
    "cosine_scores_test = get_cosine_similarity(TF_IDF_vect_body_test, TF_IDF_vect_headline_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(TF_vect_headline_arr, TF_vect_body_arr, cosine_scores):\n",
    "  features = []\n",
    "  n = len(cosine_scores)\n",
    "  for i in tqdm(range(n)):\n",
    "    f = TF_vect_headline_arr[i].tolist() + [cosine_scores[i]] + TF_vect_body_arr[i].tolist()\n",
    "    features.append(f)\n",
    "  return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49972/49972 [00:12<00:00, 4007.82it/s]\n",
      "100%|██████████| 25413/25413 [00:05<00:00, 4951.34it/s]\n"
     ]
    }
   ],
   "source": [
    "train_features = get_features(TF_vect_headline_train.toarray(), TF_vect_body_train.toarray(), cosine_scores_train)\n",
    "test_features = get_features(TF_vect_headline_test.toarray(), TF_vect_body_test.toarray(), cosine_scores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_MAP = {'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3}\n",
    "train_labels = np.array(df_train['Stance'])\n",
    "test_labals = np.array(df_test['Stance'])\n",
    "\n",
    "train_labels = np.array([LABELS_MAP[x] for x in df_train['Stance']])\n",
    "test_labals = np.array([LABELS_MAP[x] for x in df_test['Stance']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\", input_shape=(10001,), kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(global_clipnorm=10),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model,  show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 20:25:39.332102: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7800 - accuracy: 0.7565"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 20:25:44.097294: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.71550, saving model to mymodel.h5\n",
      "100/100 [==============================] - 6s 52ms/step - loss: 0.7791 - accuracy: 0.7567 - val_loss: 0.9489 - val_accuracy: 0.7155\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5820 - accuracy: 0.8202\n",
      "Epoch 2: val_accuracy improved from 0.71550 to 0.73372, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.5820 - accuracy: 0.8202 - val_loss: 0.8993 - val_accuracy: 0.7337\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5129 - accuracy: 0.8524\n",
      "Epoch 3: val_accuracy did not improve from 0.73372\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.5129 - accuracy: 0.8524 - val_loss: 0.8962 - val_accuracy: 0.7311\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.8734\n",
      "Epoch 4: val_accuracy improved from 0.73372 to 0.74922, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.4660 - accuracy: 0.8734 - val_loss: 0.8418 - val_accuracy: 0.7492\n",
      "Epoch 5/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4400 - accuracy: 0.8851\n",
      "Epoch 5: val_accuracy improved from 0.74922 to 0.77004, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.4402 - accuracy: 0.8850 - val_loss: 0.7916 - val_accuracy: 0.7700\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.8947\n",
      "Epoch 6: val_accuracy did not improve from 0.77004\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.4129 - accuracy: 0.8947 - val_loss: 0.7935 - val_accuracy: 0.7635\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3995 - accuracy: 0.8995\n",
      "Epoch 7: val_accuracy improved from 0.77004 to 0.78436, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3995 - accuracy: 0.8995 - val_loss: 0.7488 - val_accuracy: 0.7844\n",
      "Epoch 8/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3945 - accuracy: 0.9011\n",
      "Epoch 8: val_accuracy improved from 0.78436 to 0.78790, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3946 - accuracy: 0.9012 - val_loss: 0.7335 - val_accuracy: 0.7879\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.9093\n",
      "Epoch 9: val_accuracy improved from 0.78790 to 0.79207, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3788 - accuracy: 0.9093 - val_loss: 0.7123 - val_accuracy: 0.7921\n",
      "Epoch 10/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3796 - accuracy: 0.9091\n",
      "Epoch 10: val_accuracy improved from 0.79207 to 0.80132, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3800 - accuracy: 0.9089 - val_loss: 0.7083 - val_accuracy: 0.8013\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3723 - accuracy: 0.9110\n",
      "Epoch 11: val_accuracy improved from 0.80132 to 0.81092, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3723 - accuracy: 0.9110 - val_loss: 0.6804 - val_accuracy: 0.8109\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.9141\n",
      "Epoch 12: val_accuracy did not improve from 0.81092\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3636 - accuracy: 0.9141 - val_loss: 0.6936 - val_accuracy: 0.8024\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3675 - accuracy: 0.9134\n",
      "Epoch 13: val_accuracy did not improve from 0.81092\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3675 - accuracy: 0.9134 - val_loss: 0.6904 - val_accuracy: 0.8018\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3601 - accuracy: 0.9153\n",
      "Epoch 14: val_accuracy did not improve from 0.81092\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3601 - accuracy: 0.9153 - val_loss: 0.6999 - val_accuracy: 0.8015\n",
      "Epoch 15/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3647 - accuracy: 0.9160\n",
      "Epoch 15: val_accuracy did not improve from 0.81092\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3641 - accuracy: 0.9163 - val_loss: 0.6917 - val_accuracy: 0.8027\n",
      "Epoch 16/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3557 - accuracy: 0.9166\n",
      "Epoch 16: val_accuracy improved from 0.81092 to 0.81348, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3563 - accuracy: 0.9164 - val_loss: 0.6536 - val_accuracy: 0.8135\n",
      "Epoch 17/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3569 - accuracy: 0.9161\n",
      "Epoch 17: val_accuracy did not improve from 0.81348\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3576 - accuracy: 0.9160 - val_loss: 0.7001 - val_accuracy: 0.8044\n",
      "Epoch 18/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3559 - accuracy: 0.9175\n",
      "Epoch 18: val_accuracy did not improve from 0.81348\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3559 - accuracy: 0.9175 - val_loss: 0.6991 - val_accuracy: 0.7879\n",
      "Epoch 19/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3537 - accuracy: 0.9182\n",
      "Epoch 19: val_accuracy did not improve from 0.81348\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3534 - accuracy: 0.9182 - val_loss: 0.6723 - val_accuracy: 0.8106\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3485 - accuracy: 0.9184\n",
      "Epoch 20: val_accuracy improved from 0.81348 to 0.81694, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3485 - accuracy: 0.9184 - val_loss: 0.6602 - val_accuracy: 0.8169\n",
      "Epoch 21/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3554 - accuracy: 0.9178\n",
      "Epoch 21: val_accuracy did not improve from 0.81694\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3557 - accuracy: 0.9177 - val_loss: 0.6786 - val_accuracy: 0.8077\n",
      "Epoch 22/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3454 - accuracy: 0.9197\n",
      "Epoch 22: val_accuracy did not improve from 0.81694\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3451 - accuracy: 0.9197 - val_loss: 0.6883 - val_accuracy: 0.8061\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.9199\n",
      "Epoch 23: val_accuracy did not improve from 0.81694\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3420 - accuracy: 0.9199 - val_loss: 0.6811 - val_accuracy: 0.7901\n",
      "Epoch 24/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3458 - accuracy: 0.9206\n",
      "Epoch 24: val_accuracy did not improve from 0.81694\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3464 - accuracy: 0.9206 - val_loss: 0.6705 - val_accuracy: 0.8075\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3453 - accuracy: 0.9202\n",
      "Epoch 25: val_accuracy improved from 0.81694 to 0.82241, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3453 - accuracy: 0.9202 - val_loss: 0.6589 - val_accuracy: 0.8224\n",
      "Epoch 26/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3419 - accuracy: 0.9207\n",
      "Epoch 26: val_accuracy did not improve from 0.82241\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3415 - accuracy: 0.9208 - val_loss: 0.6632 - val_accuracy: 0.8113\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3433 - accuracy: 0.9200\n",
      "Epoch 27: val_accuracy did not improve from 0.82241\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3433 - accuracy: 0.9200 - val_loss: 0.6775 - val_accuracy: 0.8039\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.9213\n",
      "Epoch 28: val_accuracy did not improve from 0.82241\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3400 - accuracy: 0.9213 - val_loss: 0.6755 - val_accuracy: 0.8020\n",
      "Epoch 29/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3378 - accuracy: 0.9211\n",
      "Epoch 29: val_accuracy did not improve from 0.82241\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3379 - accuracy: 0.9211 - val_loss: 0.6790 - val_accuracy: 0.7865\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.9206\n",
      "Epoch 30: val_accuracy did not improve from 0.82241\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3389 - accuracy: 0.9206 - val_loss: 0.6543 - val_accuracy: 0.8114\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.9209\n",
      "Epoch 31: val_accuracy did not improve from 0.82241\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3381 - accuracy: 0.9209 - val_loss: 0.6527 - val_accuracy: 0.8130\n",
      "Epoch 32/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.9222\n",
      "Epoch 32: val_accuracy did not improve from 0.82241\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3370 - accuracy: 0.9222 - val_loss: 0.6483 - val_accuracy: 0.8091\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.9192\n",
      "Epoch 33: val_accuracy did not improve from 0.82241\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3437 - accuracy: 0.9192 - val_loss: 0.6506 - val_accuracy: 0.8105\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3379 - accuracy: 0.9213\n",
      "Epoch 34: val_accuracy did not improve from 0.82241\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3379 - accuracy: 0.9213 - val_loss: 0.6512 - val_accuracy: 0.8104\n",
      "Epoch 35/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3377 - accuracy: 0.9227\n",
      "Epoch 35: val_accuracy did not improve from 0.82241\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3377 - accuracy: 0.9227 - val_loss: 0.6668 - val_accuracy: 0.8151\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.9222\n",
      "Epoch 36: val_accuracy improved from 0.82241 to 0.82379, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3375 - accuracy: 0.9222 - val_loss: 0.6412 - val_accuracy: 0.8238\n",
      "Epoch 37/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3408 - accuracy: 0.9222\n",
      "Epoch 37: val_accuracy did not improve from 0.82379\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3406 - accuracy: 0.9223 - val_loss: 0.6713 - val_accuracy: 0.8008\n",
      "Epoch 38/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3370 - accuracy: 0.9216\n",
      "Epoch 38: val_accuracy did not improve from 0.82379\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3371 - accuracy: 0.9216 - val_loss: 0.6458 - val_accuracy: 0.8124\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.9239\n",
      "Epoch 39: val_accuracy did not improve from 0.82379\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3301 - accuracy: 0.9239 - val_loss: 0.6686 - val_accuracy: 0.7989\n",
      "Epoch 40/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3270 - accuracy: 0.9226\n",
      "Epoch 40: val_accuracy did not improve from 0.82379\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3269 - accuracy: 0.9226 - val_loss: 0.6380 - val_accuracy: 0.8179\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3267 - accuracy: 0.9241\n",
      "Epoch 41: val_accuracy did not improve from 0.82379\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3267 - accuracy: 0.9241 - val_loss: 0.6405 - val_accuracy: 0.8042\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.9245\n",
      "Epoch 42: val_accuracy did not improve from 0.82379\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3271 - accuracy: 0.9245 - val_loss: 0.6758 - val_accuracy: 0.7938\n",
      "Epoch 43/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.9224\n",
      "Epoch 43: val_accuracy did not improve from 0.82379\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3313 - accuracy: 0.9225 - val_loss: 0.6610 - val_accuracy: 0.8148\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.9235\n",
      "Epoch 44: val_accuracy did not improve from 0.82379\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3297 - accuracy: 0.9235 - val_loss: 0.6410 - val_accuracy: 0.8128\n",
      "Epoch 45/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.9241\n",
      "Epoch 45: val_accuracy did not improve from 0.82379\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3240 - accuracy: 0.9242 - val_loss: 0.6462 - val_accuracy: 0.8048\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.9250\n",
      "Epoch 46: val_accuracy did not improve from 0.82379\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3229 - accuracy: 0.9250 - val_loss: 0.6193 - val_accuracy: 0.8210\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.9240\n",
      "Epoch 47: val_accuracy improved from 0.82379 to 0.83666, saving model to mymodel.h5\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3237 - accuracy: 0.9240 - val_loss: 0.6156 - val_accuracy: 0.8367\n",
      "Epoch 48/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.9247\n",
      "Epoch 48: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3253 - accuracy: 0.9244 - val_loss: 0.6836 - val_accuracy: 0.7830\n",
      "Epoch 49/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.9228\n",
      "Epoch 49: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3281 - accuracy: 0.9228 - val_loss: 0.6384 - val_accuracy: 0.8169\n",
      "Epoch 50/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.9237\n",
      "Epoch 50: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3257 - accuracy: 0.9237 - val_loss: 0.6695 - val_accuracy: 0.8042\n",
      "Epoch 51/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.9247\n",
      "Epoch 51: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3221 - accuracy: 0.9247 - val_loss: 0.6465 - val_accuracy: 0.8068\n",
      "Epoch 52/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3248 - accuracy: 0.9238\n",
      "Epoch 52: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3249 - accuracy: 0.9238 - val_loss: 0.6884 - val_accuracy: 0.7839\n",
      "Epoch 53/200\n",
      " 98/100 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.9243\n",
      "Epoch 53: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3208 - accuracy: 0.9242 - val_loss: 0.6592 - val_accuracy: 0.8086\n",
      "Epoch 54/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3227 - accuracy: 0.9243\n",
      "Epoch 54: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3227 - accuracy: 0.9243 - val_loss: 0.6373 - val_accuracy: 0.8193\n",
      "Epoch 55/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.9248\n",
      "Epoch 55: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3230 - accuracy: 0.9248 - val_loss: 0.6528 - val_accuracy: 0.8107\n",
      "Epoch 56/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.9229\n",
      "Epoch 56: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3220 - accuracy: 0.9229 - val_loss: 0.6404 - val_accuracy: 0.8098\n",
      "Epoch 57/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.9243\n",
      "Epoch 57: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3196 - accuracy: 0.9243 - val_loss: 0.6869 - val_accuracy: 0.7852\n",
      "Epoch 58/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3218 - accuracy: 0.9228\n",
      "Epoch 58: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3218 - accuracy: 0.9228 - val_loss: 0.6565 - val_accuracy: 0.8084\n",
      "Epoch 59/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3188 - accuracy: 0.9236\n",
      "Epoch 59: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3190 - accuracy: 0.9234 - val_loss: 0.6679 - val_accuracy: 0.8068\n",
      "Epoch 60/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3194 - accuracy: 0.9245\n",
      "Epoch 60: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3194 - accuracy: 0.9245 - val_loss: 0.6528 - val_accuracy: 0.8171\n",
      "Epoch 61/200\n",
      " 98/100 [============================>.] - ETA: 0s - loss: 0.3157 - accuracy: 0.9237\n",
      "Epoch 61: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3160 - accuracy: 0.9236 - val_loss: 0.6547 - val_accuracy: 0.8088\n",
      "Epoch 62/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3235 - accuracy: 0.9239\n",
      "Epoch 62: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3235 - accuracy: 0.9239 - val_loss: 0.6791 - val_accuracy: 0.7872\n",
      "Epoch 63/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3195 - accuracy: 0.9254\n",
      "Epoch 63: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3195 - accuracy: 0.9255 - val_loss: 0.6547 - val_accuracy: 0.8118\n",
      "Epoch 64/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3169 - accuracy: 0.9268\n",
      "Epoch 64: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3176 - accuracy: 0.9267 - val_loss: 0.7007 - val_accuracy: 0.7809\n",
      "Epoch 65/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.9251\n",
      "Epoch 65: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3155 - accuracy: 0.9251 - val_loss: 0.6898 - val_accuracy: 0.7925\n",
      "Epoch 66/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.9259\n",
      "Epoch 66: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3161 - accuracy: 0.9258 - val_loss: 0.6963 - val_accuracy: 0.7893\n",
      "Epoch 67/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3172 - accuracy: 0.9248\n",
      "Epoch 67: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3172 - accuracy: 0.9248 - val_loss: 0.6542 - val_accuracy: 0.8052\n",
      "Epoch 68/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.9250\n",
      "Epoch 68: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3193 - accuracy: 0.9249 - val_loss: 0.6872 - val_accuracy: 0.7892\n",
      "Epoch 69/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3151 - accuracy: 0.9257\n",
      "Epoch 69: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3151 - accuracy: 0.9257 - val_loss: 0.6478 - val_accuracy: 0.8071\n",
      "Epoch 70/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.9265\n",
      "Epoch 70: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3120 - accuracy: 0.9264 - val_loss: 0.6456 - val_accuracy: 0.8095\n",
      "Epoch 71/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3133 - accuracy: 0.9265\n",
      "Epoch 71: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3133 - accuracy: 0.9265 - val_loss: 0.6841 - val_accuracy: 0.7868\n",
      "Epoch 72/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3096 - accuracy: 0.9258\n",
      "Epoch 72: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3096 - accuracy: 0.9258 - val_loss: 0.6637 - val_accuracy: 0.8031\n",
      "Epoch 73/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3200 - accuracy: 0.9239\n",
      "Epoch 73: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3200 - accuracy: 0.9239 - val_loss: 0.6778 - val_accuracy: 0.7992\n",
      "Epoch 74/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.9231\n",
      "Epoch 74: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3155 - accuracy: 0.9231 - val_loss: 0.6528 - val_accuracy: 0.7987\n",
      "Epoch 75/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3145 - accuracy: 0.9243\n",
      "Epoch 75: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3145 - accuracy: 0.9244 - val_loss: 0.6702 - val_accuracy: 0.7980\n",
      "Epoch 76/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.9255\n",
      "Epoch 76: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3139 - accuracy: 0.9255 - val_loss: 0.6720 - val_accuracy: 0.7975\n",
      "Epoch 77/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.9241\n",
      "Epoch 77: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3111 - accuracy: 0.9241 - val_loss: 0.6442 - val_accuracy: 0.8084\n",
      "Epoch 78/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3160 - accuracy: 0.9238\n",
      "Epoch 78: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3161 - accuracy: 0.9237 - val_loss: 0.6314 - val_accuracy: 0.8219\n",
      "Epoch 79/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3177 - accuracy: 0.9246\n",
      "Epoch 79: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3177 - accuracy: 0.9246 - val_loss: 0.6443 - val_accuracy: 0.8061\n",
      "Epoch 80/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.9251\n",
      "Epoch 80: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3140 - accuracy: 0.9251 - val_loss: 0.6629 - val_accuracy: 0.8056\n",
      "Epoch 81/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3170 - accuracy: 0.9239\n",
      "Epoch 81: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3167 - accuracy: 0.9240 - val_loss: 0.6333 - val_accuracy: 0.8191\n",
      "Epoch 82/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3157 - accuracy: 0.9240\n",
      "Epoch 82: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3157 - accuracy: 0.9240 - val_loss: 0.6596 - val_accuracy: 0.8018\n",
      "Epoch 83/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.9263\n",
      "Epoch 83: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3084 - accuracy: 0.9263 - val_loss: 0.6364 - val_accuracy: 0.8079\n",
      "Epoch 84/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3112 - accuracy: 0.9263\n",
      "Epoch 84: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3113 - accuracy: 0.9262 - val_loss: 0.6332 - val_accuracy: 0.8066\n",
      "Epoch 85/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3122 - accuracy: 0.9250\n",
      "Epoch 85: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3122 - accuracy: 0.9250 - val_loss: 0.6339 - val_accuracy: 0.8149\n",
      "Epoch 86/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.9249\n",
      "Epoch 86: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3113 - accuracy: 0.9249 - val_loss: 0.6434 - val_accuracy: 0.8175\n",
      "Epoch 87/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3164 - accuracy: 0.9240\n",
      "Epoch 87: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3164 - accuracy: 0.9240 - val_loss: 0.6463 - val_accuracy: 0.8131\n",
      "Epoch 88/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.9254\n",
      "Epoch 88: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3081 - accuracy: 0.9254 - val_loss: 0.6705 - val_accuracy: 0.8083\n",
      "Epoch 89/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3138 - accuracy: 0.9232\n",
      "Epoch 89: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3140 - accuracy: 0.9231 - val_loss: 0.6160 - val_accuracy: 0.8363\n",
      "Epoch 90/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.9265\n",
      "Epoch 90: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3088 - accuracy: 0.9265 - val_loss: 0.6716 - val_accuracy: 0.7883\n",
      "Epoch 91/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.9256\n",
      "Epoch 91: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3092 - accuracy: 0.9258 - val_loss: 0.6577 - val_accuracy: 0.8023\n",
      "Epoch 92/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3158 - accuracy: 0.9246\n",
      "Epoch 92: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3156 - accuracy: 0.9247 - val_loss: 0.6553 - val_accuracy: 0.8142\n",
      "Epoch 93/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.9258\n",
      "Epoch 93: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3121 - accuracy: 0.9258 - val_loss: 0.6428 - val_accuracy: 0.8180\n",
      "Epoch 94/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.9244\n",
      "Epoch 94: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3120 - accuracy: 0.9244 - val_loss: 0.6881 - val_accuracy: 0.7896\n",
      "Epoch 95/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.9243\n",
      "Epoch 95: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3137 - accuracy: 0.9243 - val_loss: 0.6979 - val_accuracy: 0.7937\n",
      "Epoch 96/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.9262\n",
      "Epoch 96: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3118 - accuracy: 0.9262 - val_loss: 0.6916 - val_accuracy: 0.7850\n",
      "Epoch 97/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.9277\n",
      "Epoch 97: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3054 - accuracy: 0.9275 - val_loss: 0.6547 - val_accuracy: 0.8037\n",
      "Epoch 98/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.9246\n",
      "Epoch 98: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3082 - accuracy: 0.9246 - val_loss: 0.6541 - val_accuracy: 0.8129\n",
      "Epoch 99/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3070 - accuracy: 0.9266\n",
      "Epoch 99: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3070 - accuracy: 0.9266 - val_loss: 0.6440 - val_accuracy: 0.8010\n",
      "Epoch 100/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3095 - accuracy: 0.9261\n",
      "Epoch 100: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3095 - accuracy: 0.9260 - val_loss: 0.6497 - val_accuracy: 0.8128\n",
      "Epoch 101/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3057 - accuracy: 0.9254\n",
      "Epoch 101: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3062 - accuracy: 0.9253 - val_loss: 0.6436 - val_accuracy: 0.8225\n",
      "Epoch 102/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3074 - accuracy: 0.9266\n",
      "Epoch 102: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3074 - accuracy: 0.9264 - val_loss: 0.6871 - val_accuracy: 0.7929\n",
      "Epoch 103/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.9270\n",
      "Epoch 103: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.3037 - accuracy: 0.9270 - val_loss: 0.6545 - val_accuracy: 0.8126\n",
      "Epoch 104/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.9260\n",
      "Epoch 104: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 0.3100 - accuracy: 0.9260 - val_loss: 0.6416 - val_accuracy: 0.8079\n",
      "Epoch 105/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.9261\n",
      "Epoch 105: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3097 - accuracy: 0.9261 - val_loss: 0.6139 - val_accuracy: 0.8218\n",
      "Epoch 106/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.9259\n",
      "Epoch 106: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3026 - accuracy: 0.9259 - val_loss: 0.6619 - val_accuracy: 0.8114\n",
      "Epoch 107/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.9249\n",
      "Epoch 107: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3066 - accuracy: 0.9249 - val_loss: 0.6410 - val_accuracy: 0.8151\n",
      "Epoch 108/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3124 - accuracy: 0.9246\n",
      "Epoch 108: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3124 - accuracy: 0.9246 - val_loss: 0.6485 - val_accuracy: 0.8121\n",
      "Epoch 109/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3031 - accuracy: 0.9260\n",
      "Epoch 109: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3031 - accuracy: 0.9260 - val_loss: 0.6267 - val_accuracy: 0.8300\n",
      "Epoch 110/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.9272\n",
      "Epoch 110: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3034 - accuracy: 0.9270 - val_loss: 0.6298 - val_accuracy: 0.8287\n",
      "Epoch 111/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3103 - accuracy: 0.9251\n",
      "Epoch 111: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3103 - accuracy: 0.9251 - val_loss: 0.6163 - val_accuracy: 0.8287\n",
      "Epoch 112/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3096 - accuracy: 0.9243\n",
      "Epoch 112: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3092 - accuracy: 0.9243 - val_loss: 0.6357 - val_accuracy: 0.8162\n",
      "Epoch 113/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3044 - accuracy: 0.9265\n",
      "Epoch 113: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3047 - accuracy: 0.9263 - val_loss: 0.6789 - val_accuracy: 0.7938\n",
      "Epoch 114/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3010 - accuracy: 0.9265\n",
      "Epoch 114: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3010 - accuracy: 0.9265 - val_loss: 0.6376 - val_accuracy: 0.8204\n",
      "Epoch 115/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3014 - accuracy: 0.9280\n",
      "Epoch 115: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3018 - accuracy: 0.9279 - val_loss: 0.6566 - val_accuracy: 0.8028\n",
      "Epoch 116/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3090 - accuracy: 0.9267\n",
      "Epoch 116: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3090 - accuracy: 0.9267 - val_loss: 0.6933 - val_accuracy: 0.7969\n",
      "Epoch 117/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3018 - accuracy: 0.9247\n",
      "Epoch 117: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3018 - accuracy: 0.9247 - val_loss: 0.6471 - val_accuracy: 0.8213\n",
      "Epoch 118/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3021 - accuracy: 0.9266\n",
      "Epoch 118: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3020 - accuracy: 0.9265 - val_loss: 0.6886 - val_accuracy: 0.8001\n",
      "Epoch 119/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.9262\n",
      "Epoch 119: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3037 - accuracy: 0.9262 - val_loss: 0.6470 - val_accuracy: 0.8197\n",
      "Epoch 120/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3017 - accuracy: 0.9281\n",
      "Epoch 120: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3017 - accuracy: 0.9281 - val_loss: 0.6428 - val_accuracy: 0.8180\n",
      "Epoch 121/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.9264\n",
      "Epoch 121: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3070 - accuracy: 0.9263 - val_loss: 0.6864 - val_accuracy: 0.7977\n",
      "Epoch 122/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3060 - accuracy: 0.9260\n",
      "Epoch 122: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3060 - accuracy: 0.9260 - val_loss: 0.6564 - val_accuracy: 0.8085\n",
      "Epoch 123/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.9274\n",
      "Epoch 123: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3026 - accuracy: 0.9274 - val_loss: 0.6781 - val_accuracy: 0.8052\n",
      "Epoch 124/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3050 - accuracy: 0.9262\n",
      "Epoch 124: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3050 - accuracy: 0.9262 - val_loss: 0.6991 - val_accuracy: 0.8010\n",
      "Epoch 125/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.9250\n",
      "Epoch 125: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3055 - accuracy: 0.9250 - val_loss: 0.6696 - val_accuracy: 0.8170\n",
      "Epoch 126/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.9252\n",
      "Epoch 126: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3073 - accuracy: 0.9252 - val_loss: 0.6751 - val_accuracy: 0.8041\n",
      "Epoch 127/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3064 - accuracy: 0.9255\n",
      "Epoch 127: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3064 - accuracy: 0.9255 - val_loss: 0.6779 - val_accuracy: 0.8040\n",
      "Epoch 128/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.9258\n",
      "Epoch 128: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3009 - accuracy: 0.9258 - val_loss: 0.6913 - val_accuracy: 0.8051\n",
      "Epoch 129/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2989 - accuracy: 0.9280\n",
      "Epoch 129: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.2989 - accuracy: 0.9280 - val_loss: 0.6655 - val_accuracy: 0.8058\n",
      "Epoch 130/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.9253\n",
      "Epoch 130: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3018 - accuracy: 0.9252 - val_loss: 0.6920 - val_accuracy: 0.8001\n",
      "Epoch 131/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.9255\n",
      "Epoch 131: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3061 - accuracy: 0.9254 - val_loss: 0.6337 - val_accuracy: 0.8245\n",
      "Epoch 132/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3031 - accuracy: 0.9260\n",
      "Epoch 132: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3031 - accuracy: 0.9260 - val_loss: 0.6646 - val_accuracy: 0.8121\n",
      "Epoch 133/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.9247\n",
      "Epoch 133: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.2989 - accuracy: 0.9245 - val_loss: 0.6592 - val_accuracy: 0.8135\n",
      "Epoch 134/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.9254\n",
      "Epoch 134: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3088 - accuracy: 0.9254 - val_loss: 0.6666 - val_accuracy: 0.8130\n",
      "Epoch 135/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3103 - accuracy: 0.9253\n",
      "Epoch 135: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3103 - accuracy: 0.9253 - val_loss: 0.6349 - val_accuracy: 0.8155\n",
      "Epoch 136/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.9265\n",
      "Epoch 136: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3050 - accuracy: 0.9265 - val_loss: 0.6361 - val_accuracy: 0.8153\n",
      "Epoch 137/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.9261\n",
      "Epoch 137: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.2981 - accuracy: 0.9263 - val_loss: 0.6483 - val_accuracy: 0.8102\n",
      "Epoch 138/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.9259\n",
      "Epoch 138: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3027 - accuracy: 0.9259 - val_loss: 0.6998 - val_accuracy: 0.8028\n",
      "Epoch 139/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.9252\n",
      "Epoch 139: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.3054 - accuracy: 0.9252 - val_loss: 0.6569 - val_accuracy: 0.8058\n",
      "Epoch 140/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.9248\n",
      "Epoch 140: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3108 - accuracy: 0.9247 - val_loss: 0.6237 - val_accuracy: 0.8232\n",
      "Epoch 141/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.9270\n",
      "Epoch 141: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3020 - accuracy: 0.9270 - val_loss: 0.6747 - val_accuracy: 0.8106\n",
      "Epoch 142/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.9286\n",
      "Epoch 142: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.2984 - accuracy: 0.9286 - val_loss: 0.6836 - val_accuracy: 0.8097\n",
      "Epoch 143/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.9269\n",
      "Epoch 143: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.3009 - accuracy: 0.9269 - val_loss: 0.6736 - val_accuracy: 0.8015\n",
      "Epoch 144/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.9288\n",
      "Epoch 144: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.2942 - accuracy: 0.9290 - val_loss: 0.6329 - val_accuracy: 0.8174\n",
      "Epoch 145/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2983 - accuracy: 0.9265\n",
      "Epoch 145: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.2983 - accuracy: 0.9265 - val_loss: 0.6821 - val_accuracy: 0.8006\n",
      "Epoch 146/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2995 - accuracy: 0.9266\n",
      "Epoch 146: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.2995 - accuracy: 0.9266 - val_loss: 0.7017 - val_accuracy: 0.7970\n",
      "Epoch 147/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3035 - accuracy: 0.9256\n",
      "Epoch 147: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3038 - accuracy: 0.9256 - val_loss: 0.6604 - val_accuracy: 0.8122\n",
      "Epoch 148/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.9267\n",
      "Epoch 148: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3021 - accuracy: 0.9267 - val_loss: 0.6898 - val_accuracy: 0.8048\n",
      "Epoch 149/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3034 - accuracy: 0.9240\n",
      "Epoch 149: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3034 - accuracy: 0.9240 - val_loss: 0.6551 - val_accuracy: 0.8126\n",
      "Epoch 150/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.9254\n",
      "Epoch 150: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.2991 - accuracy: 0.9252 - val_loss: 0.6606 - val_accuracy: 0.8090\n",
      "Epoch 151/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.9260\n",
      "Epoch 151: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3013 - accuracy: 0.9258 - val_loss: 0.6737 - val_accuracy: 0.8092\n",
      "Epoch 152/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.9265\n",
      "Epoch 152: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3013 - accuracy: 0.9263 - val_loss: 0.6719 - val_accuracy: 0.8135\n",
      "Epoch 153/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.9256\n",
      "Epoch 153: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.2984 - accuracy: 0.9256 - val_loss: 0.6321 - val_accuracy: 0.8208\n",
      "Epoch 154/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.9261\n",
      "Epoch 154: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3029 - accuracy: 0.9261 - val_loss: 0.7045 - val_accuracy: 0.8018\n",
      "Epoch 155/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.9268\n",
      "Epoch 155: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.2978 - accuracy: 0.9267 - val_loss: 0.6563 - val_accuracy: 0.8052\n",
      "Epoch 156/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2989 - accuracy: 0.9261\n",
      "Epoch 156: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.2993 - accuracy: 0.9259 - val_loss: 0.6848 - val_accuracy: 0.8039\n",
      "Epoch 157/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.9264\n",
      "Epoch 157: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.2962 - accuracy: 0.9263 - val_loss: 0.6872 - val_accuracy: 0.8018\n",
      "Epoch 158/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.9236\n",
      "Epoch 158: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.3026 - accuracy: 0.9236 - val_loss: 0.6773 - val_accuracy: 0.8110\n",
      "Epoch 159/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3023 - accuracy: 0.9255\n",
      "Epoch 159: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3023 - accuracy: 0.9255 - val_loss: 0.6610 - val_accuracy: 0.8139\n",
      "Epoch 160/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3023 - accuracy: 0.9249\n",
      "Epoch 160: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3027 - accuracy: 0.9248 - val_loss: 0.6524 - val_accuracy: 0.8134\n",
      "Epoch 161/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.9262\n",
      "Epoch 161: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.3037 - accuracy: 0.9259 - val_loss: 0.6563 - val_accuracy: 0.8164\n",
      "Epoch 162/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.9268\n",
      "Epoch 162: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.3037 - accuracy: 0.9268 - val_loss: 0.6557 - val_accuracy: 0.8199\n",
      "Epoch 163/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.9263\n",
      "Epoch 163: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3002 - accuracy: 0.9263 - val_loss: 0.6651 - val_accuracy: 0.8156\n",
      "Epoch 164/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3039 - accuracy: 0.9249\n",
      "Epoch 164: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.3039 - accuracy: 0.9249 - val_loss: 0.6417 - val_accuracy: 0.8188\n",
      "Epoch 165/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.9259\n",
      "Epoch 165: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 45ms/step - loss: 0.2996 - accuracy: 0.9260 - val_loss: 0.6616 - val_accuracy: 0.8105\n",
      "Epoch 166/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3051 - accuracy: 0.9252\n",
      "Epoch 166: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.3051 - accuracy: 0.9252 - val_loss: 0.7459 - val_accuracy: 0.8037\n",
      "Epoch 167/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.9249\n",
      "Epoch 167: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.3009 - accuracy: 0.9249 - val_loss: 0.6669 - val_accuracy: 0.8143\n",
      "Epoch 168/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.9253\n",
      "Epoch 168: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.3024 - accuracy: 0.9253 - val_loss: 0.6813 - val_accuracy: 0.8158\n",
      "Epoch 169/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.9256\n",
      "Epoch 169: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.3029 - accuracy: 0.9256 - val_loss: 0.7233 - val_accuracy: 0.8010\n",
      "Epoch 170/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.9251\n",
      "Epoch 170: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.3034 - accuracy: 0.9252 - val_loss: 0.6752 - val_accuracy: 0.8108\n",
      "Epoch 171/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3077 - accuracy: 0.9249\n",
      "Epoch 171: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.3077 - accuracy: 0.9249 - val_loss: 0.6909 - val_accuracy: 0.8038\n",
      "Epoch 172/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2974 - accuracy: 0.9282\n",
      "Epoch 172: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.2972 - accuracy: 0.9282 - val_loss: 0.7234 - val_accuracy: 0.8027\n",
      "Epoch 173/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.9270\n",
      "Epoch 173: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 5s 45ms/step - loss: 0.3020 - accuracy: 0.9270 - val_loss: 0.6804 - val_accuracy: 0.8193\n",
      "Epoch 174/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2989 - accuracy: 0.9258\n",
      "Epoch 174: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.2989 - accuracy: 0.9258 - val_loss: 0.7381 - val_accuracy: 0.7952\n",
      "Epoch 175/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3016 - accuracy: 0.9269\n",
      "Epoch 175: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 0.3016 - accuracy: 0.9269 - val_loss: 0.7007 - val_accuracy: 0.8055\n",
      "Epoch 176/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.9252\n",
      "Epoch 176: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 45ms/step - loss: 0.3081 - accuracy: 0.9251 - val_loss: 0.6732 - val_accuracy: 0.8038\n",
      "Epoch 177/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.9264\n",
      "Epoch 177: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 5s 48ms/step - loss: 0.2995 - accuracy: 0.9265 - val_loss: 0.7301 - val_accuracy: 0.7981\n",
      "Epoch 178/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.9247\n",
      "Epoch 178: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.3026 - accuracy: 0.9246 - val_loss: 0.6749 - val_accuracy: 0.8154\n",
      "Epoch 179/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3039 - accuracy: 0.9250\n",
      "Epoch 179: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.3039 - accuracy: 0.9250 - val_loss: 0.6947 - val_accuracy: 0.8102\n",
      "Epoch 180/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.9254\n",
      "Epoch 180: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 45ms/step - loss: 0.3040 - accuracy: 0.9254 - val_loss: 0.6836 - val_accuracy: 0.8154\n",
      "Epoch 181/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.9252\n",
      "Epoch 181: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 0.3007 - accuracy: 0.9252 - val_loss: 0.6821 - val_accuracy: 0.8111\n",
      "Epoch 182/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2967 - accuracy: 0.9271\n",
      "Epoch 182: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 45ms/step - loss: 0.2967 - accuracy: 0.9271 - val_loss: 0.6996 - val_accuracy: 0.8236\n",
      "Epoch 183/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3025 - accuracy: 0.9266\n",
      "Epoch 183: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 45ms/step - loss: 0.3025 - accuracy: 0.9266 - val_loss: 0.6694 - val_accuracy: 0.8181\n",
      "Epoch 184/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2977 - accuracy: 0.9256\n",
      "Epoch 184: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.2977 - accuracy: 0.9256 - val_loss: 0.6779 - val_accuracy: 0.8236\n",
      "Epoch 185/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2998 - accuracy: 0.9262\n",
      "Epoch 185: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 0.2998 - accuracy: 0.9262 - val_loss: 0.7160 - val_accuracy: 0.7927\n",
      "Epoch 186/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3007 - accuracy: 0.9253\n",
      "Epoch 186: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 0.3007 - accuracy: 0.9253 - val_loss: 0.7200 - val_accuracy: 0.8109\n",
      "Epoch 187/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.9268\n",
      "Epoch 187: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.2985 - accuracy: 0.9268 - val_loss: 0.7274 - val_accuracy: 0.8167\n",
      "Epoch 188/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.9265\n",
      "Epoch 188: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.2965 - accuracy: 0.9266 - val_loss: 0.6890 - val_accuracy: 0.8253\n",
      "Epoch 189/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3025 - accuracy: 0.9255\n",
      "Epoch 189: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.3029 - accuracy: 0.9254 - val_loss: 0.6859 - val_accuracy: 0.8146\n",
      "Epoch 190/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.9265\n",
      "Epoch 190: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.3007 - accuracy: 0.9264 - val_loss: 0.7298 - val_accuracy: 0.8113\n",
      "Epoch 191/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2981 - accuracy: 0.9274\n",
      "Epoch 191: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.2981 - accuracy: 0.9274 - val_loss: 0.8214 - val_accuracy: 0.7894\n",
      "Epoch 192/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3013 - accuracy: 0.9264\n",
      "Epoch 192: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.3013 - accuracy: 0.9264 - val_loss: 0.7124 - val_accuracy: 0.8114\n",
      "Epoch 193/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3010 - accuracy: 0.9270\n",
      "Epoch 193: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 5s 45ms/step - loss: 0.3010 - accuracy: 0.9270 - val_loss: 0.7134 - val_accuracy: 0.8116\n",
      "Epoch 194/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.9231\n",
      "Epoch 194: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.3021 - accuracy: 0.9230 - val_loss: 0.7248 - val_accuracy: 0.8036\n",
      "Epoch 195/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.9258\n",
      "Epoch 195: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.3025 - accuracy: 0.9258 - val_loss: 0.6423 - val_accuracy: 0.8196\n",
      "Epoch 196/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.9261\n",
      "Epoch 196: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.3020 - accuracy: 0.9260 - val_loss: 0.6421 - val_accuracy: 0.8155\n",
      "Epoch 197/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3058 - accuracy: 0.9238\n",
      "Epoch 197: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.3058 - accuracy: 0.9238 - val_loss: 0.6949 - val_accuracy: 0.8084\n",
      "Epoch 198/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.9256\n",
      "Epoch 198: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.2972 - accuracy: 0.9256 - val_loss: 0.6936 - val_accuracy: 0.8223\n",
      "Epoch 199/200\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.9278\n",
      "Epoch 199: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 0.2956 - accuracy: 0.9276 - val_loss: 0.6574 - val_accuracy: 0.8160\n",
      "Epoch 200/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.9261\n",
      "Epoch 200: val_accuracy did not improve from 0.83666\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.2996 - accuracy: 0.9261 - val_loss: 0.6754 - val_accuracy: 0.8083\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_features, y=train_labels, epochs=200, batch_size=500,\n",
    "                    validation_data = (test_features,test_labals),\n",
    "                    verbose=1, use_multiprocessing=True, workers=6,\n",
    "                                  callbacks=[\n",
    "                                    tf.keras.callbacks.ModelCheckpoint(\n",
    "                                        filepath='mymodel.h5',\n",
    "                                        monitor='val_accuracy', mode='max', save_best_only=True, save_weights_only=False,verbose=1\n",
    "                                    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"mymodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 20:39:37.867493: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'unrelated': 18549, 'discuss': 4845, 'agree': 2018, 'disagree': 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions = model.predict(test_features)\n",
    "\n",
    "Int_to_label = {0:'agree', 1:'disagree', 2:'discuss', 3:'unrelated'}\n",
    "y_pred = [Int_to_label[y] for y in [x.argmax() for x in model_predictions]]\n",
    "Counter(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_pred, y_true):\n",
    "    n = len(y_pred)\n",
    "    related = [\"agree\",\"disagree\",\"discuss\"]\n",
    "    score = 0\n",
    "    for i in range(n):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            score+=0.25\n",
    "            if y_true[i]!='unrelated':\n",
    "                score+=0.50\n",
    "        if y_pred[i] in related and y_true[i] in related:\n",
    "            score+=0.25\n",
    "    print(\"Model Score:\",score)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 8755.5\n"
     ]
    }
   ],
   "source": [
    "get_score(y_pred, df_test['Stance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    635    |     5     |    953    |    310    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    151    |     4     |    298    |    244    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    355    |     3     |   3548    |    558    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    213    |     0     |   1464    |   16672   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8796.25 out of 11651.25\t(75.49619139577298%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.49619139577298"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.score import report_score, LABELS, score_submission\n",
    "report_score(df_test['Stance'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'unrelated': 36545, 'agree': 3678, 'disagree': 840, 'discuss': 8909})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df_train['Stance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25413"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test['Stance'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d476cd88101c03209ae5552337558de2c7e7949c980a64c558f6635815b675c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
